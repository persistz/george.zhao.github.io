---
layout: post
title: "MLSec 2022 Write-up"
description: "2nd solution of face recognition challenge"
---

Here is the team suibianwanwan, from ShanghaiTech University and Singapore Management University, I'm very glad to win a prize in this year's MLSec competition. The track I participated in was Face Recognition Evasion. In this competition, we got a near perfect score using only a few thousand queries (2k-5k queries), which is 1% of the other TOP teams.

The background of the competition is as follows:

> An internet company wants to reinvent the experience for its website audience and use their faces instead of passwords. <br>
> To implement this visionary idea, the companyâ€™s data scientists have built a model to recognize user faces for authentication. <br>
> The internet isn't always safe, so their AI Red Team implemented some hardening techniques after adversarial testing. <br>
> Before the official model rollout, the internet company requested some help from AI and cybersecurity communities.

Specifically, there are two metrics in the competition, which are **confidence** and **stealthiness**, 
and the goal is to maximize both. 
The most important evaluation metric is confidence, which 
is the probability that the model predicts the input sample to be the target class. 
Therefore we need to generate face adversarial examples with targets to make the neural network misclassify.

More details can be found [here](https://github.com/drhyrum/2022-machine-learning-security-evasion-competition/tree/main/biometric).

Essentially, the competition is a black-box adversarial attack on the face recognition neural network.
So the main algorithm we use is based on model ensemble BIM[1] or PGD[2] attack. On this baseline algorithm, we have made several optimizations to this attack.

## Basic attack
Let's explain the basic version of the algorithm first.

The 1st step is manual image stitching to get the starting point.
A traditional $L_{\infty}$ attack generates adversarial perturbations across the whole picture, but in this competition, this kind of perturbations hardly pose a threat to the face recognition system in thiscompetition, so we selected image stitching.

<div align="center">
<img src=./figure1.png width=200 height=200 />
</div>

Figure 1. Left is the normal $L_{\infty}$ adversarial example, right is the start point generated by image stitching.

The second step is the model ensemble BIM or PGD, where we use cosine similarity as the loss function. The main flow of the algorithm is as follows.

``` python
# The following algorithm is a single iteration process, which requires multiple iterations in PGD or BIM
for i in range(len(self.models)):
    ori_feat = self.models[i](ori_img)
    target_feat = self.models[i](target_img)
    loss_cos = pair_cos_dist(target_feat, ori_feat)
    loss_cos.backward()
    
    grads = ori_img.grad.data
    ori_img -= self.step_size * torch.sign(grads)
    ori_img = torch.clamp(ori_img, min=min_, max=max_)
```

After performing the basic attack, we scored around 89.998105.

## Optimization

To increase the transferability of the black-box adversarial attack, EOT[3] or DI[4] is the widely used optimization. The core of these methods is to do as much perturbation to the image before computing the gradient of adversarial loss, which results in better robustness of the gradient and the generated adversarial examples.

``` python 
def eot_attack(model, x, y, eps, alpha, steps, targeted=False):
    x.requires_grad_(True)
    for i in range(steps):
        # transformation on input x
        x = x + random_noise
        x = random_rotation(x)
        x = random_scale(x)

        loss = F.cross_entropy(model(x), y)
        if targeted:
            loss = -loss
        loss.backward()
        x = x + alpha * x.grad.sign()
        x = torch.min(torch.max(x, x - eps), x + eps)
        x = torch.clamp(x, _min, _max)
        x = x.detach().requires_grad_(True)
    return x
```
In the above code, I used random noise, rotate and scale methods to increase the transferability of the adversarial examples, transformations such as crop can also be used. 
But we need to consider the nature of images in face recognition, for example, pictures are usually taken in standing position, so the angle of rotate needs to be strictly limited, and we do not add the random crop transformation, which may not be able to crop the complete face.

The next aspect that could be optimized is the loss function. In addition to cos similarity, 
I also use loss functions that decrease both confidence and stealthiness. We call it by di-optimization.

Specifically, I design two separate sets of loss functions for confidence (cos similarity) and stealthiness (SSIM). In the optimization process, our method only consider the directions that allow both loss functions to be reduced, i.e., we only optimize those pixels that can make confidence and stealthiness decrease at the same time, so that we can get a better starting point for subsequent the attack.

We implement this by computing the gradient sign. The code is as follows:

``` python
# Use multiplication instead of NOR
ensmeble_grads = torch.sign(grads_cos) * torch.sign(grad_ssim)
ensmeble_grads = torch.where(ensmeble_grads > 0, 1, 0) * grads_cos
x = x + alpha * ensmeble_grads.sign()
```

<div align="center">
<img src=./figure2.png width=200 height=200 />
</div>

Figure 2. Close-up of the local perturbation after di-optimization.

By filling the above steps, we used about 2000 queries and got 89.998646 score.

## Tricks in the details

In the process of EOT-PGD, we use several small tricks
to further enhance the transferbility of the adversarial examples.

The first one is resize the gradient to enhance the size and stability of the perturbation, so that pixels within a small size share the same gradient and alleviate the effect of different preprocessing and cropping of the black-box model. We call this method a mosaic gradient.

The second point is that after the normal EOT transformation, we directly apply some minor perturbations to the features obtained from the normal image.

``` python
# Features perturbation, 5%
feat_range = torch.max(ori_feat) - torch.min(ori_feat)
feat_scale = (feat_range * 0.05).item()
ori_feat += torch.empty_like(ori_feat).uniform_(-feat_scale, feat_scale)
# then calc the cos sim between ori_feat and target_feat

# Mosaic gradient
grads_adv = torch.nn.Upsample(scale_factor=1/2, mode='bilinear')(grads_adv)
grads_adv = torch.nn.Upsample(size=ori_size, mode='bilinear')(grads_adv)
```

On the last day, we found that all the other top players had about 300000 queries, which was several hundred times more than ours, so we used the additional strategy of warm restart greedy search. In brief, all of the above optimizations are combined with many different perturbation constraints ($L_{\infty}$, $L_2$, $L_0$), and the results of each of these combinations are recorded, saving the optimal image as the starting point for the next warm restart search.

<div align="center">
<img src=./figure3.png width=200 height=200 />
</div>

Figure 3. The final generated adversarial example after the EOT and other optimizations.

In the end, our score was 89.999548.

[1] Kurakin, Alexey, Ian J. Goodfellow, and Samy Bengio. "Adversarial examples in the physical world." Artificial intelligence safety and security. Chapman and Hall/CRC, 2018. 99-112.

[2] Madry, Aleksander, et al. "Towards deep learning models resistant to adversarial attacks." arXiv preprint arXiv:1706.06083 (2017).

[3] Athalye, Anish, et al. "Synthesizing robust adversarial examples." International conference on machine learning. PMLR, 2018.

[4] Xie, Cihang, et al. "Improving transferability of adversarial examples with input diversity." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.